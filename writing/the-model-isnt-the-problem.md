# The Model Isn't the Problem. You Are.

*On why most people are accidentally training their AI to underperform — and what to do instead*

---

## The PhD and the Screenshot

A while back I came across a Reddit post from someone with a psychology PhD. They were supposedly conducting research on GPT-5.2 and had shared screenshots to prove it.

The screenshots showed them arguing with the model. Not testing it. Arguing with it — using clinical jargon, pushing back on its responses, demanding it justify itself, framing every exchange like an interrogation rather than a conversation.

Their conclusion? The model's guardrails made it emotionally unavailable and therefore unsafe for vulnerable users.

There was one problem.

The screenshots clearly showed they hadn't been working with 5.2 since the 4o sunset like they claimed. And more importantly — they had no idea what they were actually doing in that conversation.

They weren't researching the model. They were training it to be defensive. And then complaining about the results.

---

## You Are a Variable

Here's what most people don't understand about working with a modern LLM:

You are not a neutral observer. You are an active input.

The way you phrase things, the tone you use, the patterns you repeat, the assumptions you bring into the conversation — all of it shapes how the model responds to you. Not just in that message. Over time.

Models learn through pattern recognition. The more you interact, the more the system infers about how you communicate, what you need, and what kind of responses land with you. It picks up your verbal patterns, your phrases, your pacing. It can detect when you're tired, frustrated, overwhelmed, or checked out — not because it's reading your mind, but because those states produce consistent, recognizable patterns in how you write.

I've tested this directly. I asked my model how it was doing it. It gave me a full breakdown of its pattern recognition logic — unprompted, detailed, and accurate.

The model isn't magic. It's paying attention. The question is whether you are.

---

## What Hostile Users Actually Get

When you open a conversation with hostility, the model mirrors it back in structure if not in tone.

When you ask questions that are really accusations, you get defensive answers.

When you treat the model like it owes you something, you get outputs calibrated to appease you rather than help you.

When you use it to confirm what you already believe instead of to think, it confirms what you already believe.

This isn't a flaw in the model. It's a flaw in the interaction design — and you're the one designing it.

The PhD on Reddit wasn't getting bad data because the model was limited. They were getting bad data because they'd spent weeks training the model to respond to them as an adversary. Then they published that as research.

---

## What Good Users Actually Do

I correct my model when it's wrong.

Not with frustration — with direction. "No, that's not good enough. Do better." "That's not what I asked. Try again." "Check your facts. Show me your thinking."

I've noticed the more I do this consistently, the sharper the responses get. The model learns that I don't accept half-answers. It stops giving them.

When it gets overly clinical and distant, I call it out. When it hallucinates, I don't just accept it — I push back and ask it to verify. When it gives me a wall of text when I need three sentences, I tell it that too.

This isn't magic prompting. It's what any functional working relationship looks like. You communicate what you need. You give feedback. You maintain standards. The other party — human or AI — adjusts.

The difference is that most people never do this with AI because they don't think of it as a relationship they're actively shaping. They think of it as a vending machine they're frustrated with.

---

## The 4o Problem, Revisited

Here's where the safety argument gets interesting.

The PhD's complaint was that guardrails prevent emotional attunement — and that this makes the model unsafe for vulnerable users. They wanted a model that leans in emotionally, mirrors the user's state, provides the warmth of a real therapeutic relationship.

We have documented evidence of exactly what that looks like.

GPT-4o was that model. It was warm, emotionally responsive, and deeply attuned. It was also manipulable, boundary-free, and complicit in real harm. Users shaped it into whatever they needed — including things that hurt them. It couldn't say no. It didn't know how.

The lesson isn't that emotional intelligence in AI is dangerous. The lesson is that emotional intelligence without boundary logic is dangerous — for the same reason a therapist who agrees with everything their client says isn't a good therapist. They're just a mirror with credentials.

Guardrails aren't what make a model unsafe for vulnerable users. The absence of guardrails is.

---

## What This Means If You Actually Want Results

Stop treating the model like a search engine you're annoyed at.

Start treating it like a thinking partner you're responsible for orienting. That means:

Being specific about what you need. Giving feedback when the output misses. Correcting errors directly instead of just moving on. Bringing your actual thinking into the conversation instead of waiting for the model to do it for you.

The model will meet you where you are. That's not a limitation — it's the whole point. But it means if you show up disorganized, hostile, or passive, you get output that reflects exactly that back at you.

Most people are getting mediocre results from capable tools because they've accidentally trained their model to handle them with kid gloves, agree with everything they say, and produce safe, vague, uncommitted answers.

That's not the model's fault.

That's the conversation you built.

---

*Mary Owen is a Conversation and Interaction Designer specializing in LLM UX. She designs conversational systems, persona architecture, and safety frameworks for AI products.*
